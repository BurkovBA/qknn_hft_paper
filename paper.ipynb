{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cb2706-a52d-476b-a39b-be0b23228e94",
   "metadata": {},
   "source": [
    "![paper screen](img/paper_screen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3769f-83bb-4a16-a2ea-c040c7a197a8",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "* Markov decision process terminology and Q-learning\n",
    "* Problem-specific notation\n",
    "* Q-knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b86e9-e9c3-4bc1-8a16-677599c37a35",
   "metadata": {},
   "source": [
    "# Markov decision process\n",
    "\n",
    "## Notation\n",
    "\n",
    "* $Z_T$ - state-time vector space, $[0, T] Ã— E$ - state space;\n",
    "* $A_z \\in \\mathbb{A}$ - market maker control, we are seeking, consists of actions $\\alpha_t$; $mathbb{A}$ is the set of all admissible strategies;\n",
    "* $\\lambda$ - intensity of the jump;\n",
    "* $Q$\u0017- transitions kernel;\n",
    "* $r$ - reward;\n",
    "\n",
    "The process is Markov, because current state of the system is completely described by the state vector $z_t$ and depends only on the previous state and action taken.\n",
    "\n",
    "Our goal is to maximize the value function of Markov decision process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80710d4d-b7e1-4044-8300-55a51589b54a",
   "metadata": {},
   "source": [
    "# Actions and rewards in Q-learning\n",
    "\n",
    "For each state of the system $z_t$ we can select an action from a space $A_z$.\n",
    "\n",
    "By choosing an action $\\alpha_z$, we obtain a reward $r$. \n",
    "\n",
    "Hence, there is a reward function $R: Z_t \\times A_z$, which defines the rewards, associated with each action at each state.\n",
    "\n",
    "![q_learning](img/q_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6652a80-2d1c-4f49-a995-1456587a3fc3",
   "metadata": {},
   "source": [
    "# Value function, reward function, action-value function (Q-function)\n",
    "\n",
    "Value function is the estimate of the current position at the moment of time $t$ and state $z$. It represents the expectation of maximum reward you can attain, if you play your cards optimally.\n",
    "\n",
    "$V(t, z) = \\sup_{\\alpha \\in \\mathbb{A}} \\mathbb{E} \\lbrack \\int \\limits_{t}^{T} R_s(Z_s) ds \\rbrack$, where $R_s: A_z \\times Z_s \\to \\mathbb{R}$ is the reward that can be attained at the moment $s$, when the system is in a state $Z_s$.\n",
    "\n",
    "Let us decompose the reward function $R_s$ into instantaneous reward $f(\\alpha_s, Z_s)$ at step $s$ and terminal reward $g(Z_T)$: $R_s = f(\\alpha_s, Z_s) ds + g(Z_T)$.\n",
    "\n",
    "$V(t, z) = \\sup_{\\alpha \\in \\mathbb{A}} \\mathbb{E} \\lbrack \\int \\limits_{t}^{T} f(\\alpha_s, Z_s) ds + g(Z_T) \\rbrack$\n",
    "\n",
    "Let us explicitly reflect the fact that Value function depends on the action we take at each moment $s$ according to our policy/strategy.\n",
    "\n",
    "Introduce $Q_t(z, \\alpha)$ an action-value function, relfecting the terminal retrun we get upon entering the state.\n",
    "\n",
    "Optimal strategy $A_s \\in \\mathbb{A}$ produces the optimal action-value function $Q^*_t(z, \\alpha)$, such that $Q^*_t(z, \\alpha) = V(t, z)$:\n",
    "\n",
    "$Q^*_t(z_t, \\alpha_t) = f(\\alpha, z) + \\gamma \\mathbb{E}_{p(z_{t+1} | z_t, \\alpha_t)} \\lbrack \\max_{\\alpha_{t+1}} Q^*_{t_0}(z_{t+1}, \\alpha_{t+1}) | z_t, \\alpha_t \\rbrack$ , where $\\gamma$ is a discount factor for delayed reward.\n",
    "\n",
    "This recursion is Bellman's equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9720e-cf03-4e2a-a709-4c66eab4c889",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "![q-learning 2](img/q_learning2.png)\n",
    "\n",
    "Here epsilon-greedy strategy is a strategy of choice of actions, based on a balance of exploration and exploitation, which exploits the optimal action, found so far in most cases and explores new options with probability $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1175488e-d7ab-4850-bc9c-f6773f12aa75",
   "metadata": {},
   "source": [
    "# State\n",
    "\n",
    "$Z_t = X_t, Y_t, a_t, b_t, na_t, nb_t, pa_t, pb_t, ra_t,rb_t$, where:\n",
    "\n",
    "* $X_t$ - cash held by marketmaker\n",
    "* $Y_t$ - inventory of marketmaker\n",
    "* $a_t = (a_1, ..., a_K)$ - bbo ask levels 1..K\n",
    "* $b_t = (b_1, ..., b_K)$ - bbo bid levels 1..K\n",
    "* $na_t = (na_1, ..., nb_K)$ - ranks of marketmaker ask orders in the respective level's order queue\n",
    "* $nb_t = (nb_1, ..., nb_K)$ - ranks of marketmaker bid orders in the respective level's order queue\n",
    "* $pa_t$ - bbo ask price\n",
    "* $pb_t$ - bbo bid price\n",
    "* $ra_t = (ra_1, ..., ra_K)$ - market maker ask order positions\n",
    "* $rb_t = (rb_1, ..., rb_K)$ - market maker bid order positions\n",
    "\n",
    "![orders](img/orders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b290a7bf-abae-43ba-a7c8-bf01fcc64e5b",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbours regression\n",
    "\n",
    "A simple non-parametric method for regression/classification:\n",
    "\n",
    "![knn](img/knn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db929fc0-f7c8-4e08-a64e-0ea57f2ad22c",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "![algorithm](img/algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c928f26c-8780-4e69-9503-aba05c2175f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
