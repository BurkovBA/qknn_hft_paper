{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cb2706-a52d-476b-a39b-be0b23228e94",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![paper screen](img/paper_screen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3769f-83bb-4a16-a2ea-c040c7a197a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Plan\n",
    "\n",
    "* Markov decision process and Q-learning terminology\n",
    "* Problem-specific notation\n",
    "* Q-knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c2eb7-2223-4d9b-9f41-b6c9e29748e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Market making as a refinforcement learning/stochastic control problem\n",
    "\n",
    "The paper at hand approaches the problem of market making as an optimal control / reinforcement learning problem.\n",
    "\n",
    "It seeks to find an optimal quality function to describe current state of market and an optimal policy to take actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b014f7e-b460-4251-a62e-905cfe56f138",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Markov Decision Process\n",
    "\n",
    "Optimal control theory works with a probabilistic environment, the state of which is fully described by a state $Z_t$ (one might pay attention to the time $t$ or ignore it, dropping the sub-index and considering just the state $Z$, irrespective of time).\n",
    "\n",
    "The process is Markovian because transition from $Z_t$ to $Z_{t+1}$ is described by a transition probability matrix/kernel $Q$ (don't confuse it with Q-function, used later, though).\n",
    "\n",
    "The agent can take an action $\\alpha_t$ at each time step $t$, receiving a reward $r_t$ and transitioning the system into state $Z_{t+1}$. Generally the transition is probabilistic and actions can be probabilistic as well.\n",
    "\n",
    "The goal is to find an optimal strategy/policy $A$/$\\pi$ from a certain space of valid strategies $\\mathbb{A}$, which optimizes the total reward $\\sum \\limits_t r_t$ an agent gets over its lifetime.\n",
    "\n",
    "![MDP](img/MDP_agent_env.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80710d4d-b7e1-4044-8300-55a51589b54a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Actions and rewards in MDP\n",
    "\n",
    "For each state of the system $z_t$ we can select an action from a space $A_z$.\n",
    "\n",
    "By choosing an action $\\alpha_z$, we obtain a reward $r$. \n",
    "\n",
    "Hence, there is a reward function $R: Z_t \\times A_z \\to \\mathbb{R}$, which defines the rewards, associated with each action at each state.\n",
    "\n",
    "![q_learning](img/q_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6652a80-2d1c-4f49-a995-1456587a3fc3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Value function, reward function, quality function (a.k.a. Q-function, action-value function)\n",
    "\n",
    "Value function is the estimate of the current position at the moment of time $t$ and state $z$. It represents the expectation of maximum reward you can attain, if you play your cards optimally.\n",
    "\n",
    "$V(T, z) = \\sup_{\\alpha \\in \\mathbb{A}} \\mathbb{E} \\lbrack \\int \\limits_{0}^{T} R_s(Z_s) ds \\rbrack$, where $R_s: A_z \\times Z_s \\to \\mathbb{R}$ is the reward that can be attained at the moment $s$, when the system is in a state $Z_s$.\n",
    "\n",
    "Let us decompose the reward function $R_s$ into instantaneous reward $f(\\alpha_s, Z_s)$ at step $s$ and terminal reward $g(Z_T)$: $R_s = f(\\alpha_s, Z_s) ds + g(Z_T)$.\n",
    "\n",
    "$V(t, z) = \\sup_{\\alpha \\in \\mathbb{A}} \\mathbb{E} \\lbrack \\int \\limits_{t}^{T} f(\\alpha_s, Z_s) ds + g(Z_T) \\rbrack$\n",
    "\n",
    "Let us explicitly reflect the fact that Value function depends on the action $\\alpha_s$ we take at each moment $s$ according to our policy/strategy $A_s$.\n",
    "\n",
    "Introduce an action-value function $Q_t(z, \\alpha)$, relfecting the terminal return we get upon entering the state.\n",
    "\n",
    "Optimal strategy $A_s \\in \\mathbb{A}$ produces the optimal action-value function $Q^*_t(z, \\alpha)$, such that $Q^*_t(z, \\alpha) = V(t, z)$:\n",
    "\n",
    "$Q^*_t(z_t, \\alpha_t) = f(\\alpha, z) + \\gamma \\mathbb{E}_{p(z_{t+1} | z_t, \\alpha_t)} \\lbrack \\max_{\\alpha_{t+1}} Q^*_{t_0}(z_{t+1}, \\alpha_{t+1}) | z_t, \\alpha_t \\rbrack$ , where $\\gamma$ is a discount factor for delayed reward.\n",
    "\n",
    "This recursion is Bellman's equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b4708-3d75-4dde-8828-53be9e2c809e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# TD-learning, TD(0)\n",
    "\n",
    "We need to come up with a way to estimate the value function $V(Z)$ and/or quality function $Q(Z,A)$.\n",
    "\n",
    "The general approach is iterative.\n",
    "\n",
    "Recall the general first order optimization: $V^{new}(Z) = V^{old}(Z) + lr \\cdot \\Delta V(Z)$, where $lr$ is step size/learning rate and $\\Delta V$ is improvement of estimate over one step. However, upon meaningful steps your state also changes, and we need to take this into account. Substitute Bellman's equation as $\\Delta V$:\n",
    "\n",
    "$V^{new}(Z_t) = V^{old}(Z_t) + lr \\cdot \\overbrace{(\\underbrace{r_t + \\gamma V^{old}(Z_{t+1})}_{\\text{TD estimate of V}} - V^{old}(Z_t))}^{\\text{TD error}}$\n",
    "\n",
    "This is an example of Temporal Difference learning (TD-learning). Specifically, this is 1-step deep TD-learning, TD(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e788d-c65b-4f85-9fa1-45f2ed190b3d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# TD(1)\n",
    "\n",
    "We can go deeper:\n",
    "\n",
    "$V^{new}(Z_t) = V^{old}(Z_t) + lr \\cdot \\overbrace{(\\underbrace{r_t + \\gamma r_{t+1} + \\gamma^2 V^{old}(Z_{t+2})}_{\\text{TD(1) estimate of V}} - V^{old}(Z_t))}^{\\text{TD(1) error}}$\n",
    "\n",
    "One might pick arbitrary N for TD(N)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e0cd2f-d79c-442c-bc80-b673b6c10c28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Q-learning\n",
    "\n",
    "When applied to quality function instead of value function, we get:\n",
    "\n",
    "$Q^{new}(Z_t, \\alpha_t) = V^{old}(Z_t, \\alpha_t) + lr \\cdot \\overbrace{(\\underbrace{r_t + \\gamma \\max\\limits_{\\alpha} Q^{old}(Z_{t+1}, \\alpha)}_{\\text{TD estimate}} - Q^{old}(Z_t, \\alpha_t))}^{\\text{TD error}}$\n",
    "\n",
    "Importantly, $\\alpha_t$ here does **not** necesserily need to be optimal. One might observe other person take actions, get a change in quality function and learn from their mistakes.\n",
    "\n",
    "This case is called **off-policy learning** in contrast to on-policy learning, when next actions are generated according to our policy in algorithms, such as SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9720e-cf03-4e2a-a709-4c66eab4c889",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Q-learning\n",
    "\n",
    "![q-learning 2](img/q_learning2.png)\n",
    "\n",
    "Here epsilon-greedy strategy is a strategy of choice of actions, based on a balance of exploration and exploitation, which exploits the optimal action, found so far in most cases and explores new options with probability $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1175488e-d7ab-4850-bc9c-f6773f12aa75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# State space: current problem\n",
    "\n",
    "$Z_t = X_t, Y_t, a_t, b_t, na_t, nb_t, pa_t, pb_t, ra_t,rb_t$, where:\n",
    "\n",
    "* $X_t$ - cash held by marketmaker\n",
    "* $Y_t$ - inventory of marketmaker\n",
    "* $a_t = (a_1, ..., a_K)$ - bbo ask levels 1..K\n",
    "* $b_t = (b_1, ..., b_K)$ - bbo bid levels 1..K\n",
    "* $na_t = (na_1, ..., nb_K)$ - ranks of marketmaker ask orders in the respective level's order queue\n",
    "* $nb_t = (nb_1, ..., nb_K)$ - ranks of marketmaker bid orders in the respective level's order queue\n",
    "* $pa_t$ - bbo ask price\n",
    "* $pb_t$ - bbo bid price\n",
    "* $ra_t = (ra_1, ..., ra_K)$ - market maker ask order positions\n",
    "* $rb_t = (rb_1, ..., rb_K)$ - market maker bid order positions\n",
    "\n",
    "![orders](img/orders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b86e9-e9c3-4bc1-8a16-677599c37a35",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Current problem as a Markov decision process\n",
    "\n",
    "## Notation\n",
    "\n",
    "* $Z_T$ - state-time vector space, $[0, T] Ã— E$ - state space;\n",
    "* $A_z \\in \\mathbb{A}$ - market maker control, we are seeking, consists of actions $\\alpha_t$; $\\mathbb{A}$ is the set of all admissible strategies;\n",
    "* $Q$\u0017- transitions kernel of the Jump process;\n",
    "* $\\lambda$ - intensity of the jump (sum of intensities of arrivals of market, limit and cancel orders) - i.e. time intervals $\\tau$ between arrivals of orders have an exponential distribution $\\tau \\sim e^{-\\lambda t}$;\n",
    "* $r$ - reward;\n",
    "\n",
    "3 types of orders arrive: market orders, limit orders and cancel orders.\n",
    "\n",
    "Transition kernel governs the state transition process: $Q(B \\times C) = \\lambda(z) \\int \\limits_0^{T-t} e^{-\\lambda(z)s} {\\bf 1}_B(t+s) Q'(C|\\phi^\\alpha(z), \\alpha) ds + e^{-\\lambda(z)(T -t)} {\\bf 1}_{T \\in B, z \\in C}$, where $B$ is time, $C$ is state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f69dcda-3208-492f-a5b1-1789f574c8ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Infinite state space \n",
    "\n",
    "In general state space might be too large to traverse it completely. E.g. in chess the number of states possible could be $10^80$.\n",
    "\n",
    "What approaches can we take to overcome this problem?\n",
    "\n",
    "* (MC)MC approach\n",
    "* non-parametric kNN + quantization approach\n",
    "*  Reparametrization: e.g. DQN - instead of traversing the state of spaces, let us traverse a relatively low-dimensional state of Neural Network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b290a7bf-abae-43ba-a7c8-bf01fcc64e5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# k-Nearest Neighbours regression\n",
    "\n",
    "A simple non-parametric method for regression/classification:\n",
    "\n",
    "![knn](img/knn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a6fbf-59ca-4784-be89-3a5897fbdb5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Quantization\n",
    "\n",
    "In order to use qKNN, one needs to introduce a grid $\\Gamma$, which allows for quantization of state space $E$.\n",
    "\n",
    "![grid](img/grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db929fc0-f7c8-4e08-a64e-0ea57f2ad22c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Algorithm\n",
    "\n",
    "![algorithm](img/algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5268a5-96ee-4c37-8353-00b5bc889e99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Thanks for your attention\n",
    "\n",
    "This presentation is available here: https://github.com/BurkovBA/qknn_hft_paper.\n",
    "\n",
    "## References:\n",
    "* https://www.youtube.com/watch?v=BvQVovMefsM - kNN\n",
    "* https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/slides/lec21.pdf - UToronto course on Q-learning\n",
    "* https://www.youtube.com/watch?v=0iqz4tcKN58 - Steve Brunson on TD-learning and Q-learning\n",
    "* https://arxiv.org/pdf/1802.03900.pdf - qKNN\n",
    "* https://arxiv.org/pdf/2105.14464.pdf - CLVQ (quantization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
